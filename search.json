[
  {
    "objectID": "pxtextmining/model_selection.html",
    "href": "pxtextmining/model_selection.html",
    "title": "Model selection",
    "section": "",
    "text": "How do we decide which machine learning model architecture to try? There are hundreds of different types of models out there which utilise different algorithms to fit the data. As a former librarian the obvious starting point was to do a quick review of the available literature.\nThe main points of consideration when selecting a model are appropriateness for the task at hand, and the tradeoff between resource consumption and performance. For example, our multilabel dataset is very imbalanced, and the aim is to predict minority classes with good precision. Additionally, many of the most advanced algorithms at the forefront of this field also require huge computational resources - GPT-3 cost approximately US$4.6 million to train[1]\nSimple and computationally efficient algorithms like Naive Bayes, K Nearest Neighbours and Support Vector Machine may be a useful place to start and this is where we will begin our efforts, to establish a baseline.[2] Decision Trees/Random Forests might also be a useful starting point.[3]\nNeural network approaches such as Convolutional Neural Network (CNN) or Recurrent Neural Networks (RNN) such as Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU) could be tried. These could also be combined using ensemble methods.[4] Some research has shown that CNNs with Attention, or Bidirectional GRU even outperformed pretrained transformer models like BERT in some contexts.[5] Given that the training time for a CNN on the same NLP task as BERT was over 90% faster this may be a tradeoff that is necessary given the resources available for this project.[6]\nMost cutting edge research in natural language processing for text classification focuses on the use of pretrained transformers, like BERT and its various relations including RoBERTa, DistilBERT, XLNet and ALBERT.[7] This is then finetuned to the specific classification task.[8] These can take a long time to train; for example, XLNet appeared to perform best on a highly unbalanced multilabel dataset but took twice as much time (22 hours) to finetune compared to RoBERTa and ELECTRA so this is something to bear in mind when conducting model training and selection.[9]\nAs well as selecting model architecture, text preprocessing is another area that will require some experimentation. For example, the finetuning of token length, or type of word embedding used can impact on performance significantly.[5] However, in some cases, pre-processing text can also only result in marginal improvements to the F-measure when compared to text ‘as is’.[10] Using pretrained word embeddings such as GloVe or BioWordVec can improve the speed of training, although there is the risk of losing some words in the dataset that are not in the embedding vocabulary.[6]\nWe will try several different model architectures, recording the performance of each one using established performance metrics. The best model will be saved and made available for use via an API."
  },
  {
    "objectID": "pxtextmining/model_selection.html#bibliography",
    "href": "pxtextmining/model_selection.html#bibliography",
    "title": "Model selection",
    "section": "Bibliography",
    "text": "Bibliography\n[1] Li, Chuan, ‘OpenAI’s GPT-3 Language Model: A Technical Overview’, Jun. 03, 2020. https://lambdalabs.com/blog/demystifying-gpt-3 (accessed Feb. 07, 2023).\n[2] A. I. Kadhim, ‘Survey on supervised machine learning techniques for automatic text classification’, Artif Intell Rev, vol. 52, no. 1, pp. 273–292, Jun. 2019, doi: 10.1007/s10462-018-09677-1.\n[3] M.-L. Zhang and Z.-H. Zhou, ‘A Review on Multi-Label Learning Algorithms’, IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819–1837, Aug. 2014, doi: 10.1109/TKDE.2013.39.\n[4] J. Langton, K. Srihasam, and J. Jiang, ‘Comparison of Machine Learning Methods for Multi-label Classification of Nursing Education and Licensure Exam Questions’, in Proceedings of the 3rd Clinical Natural Language Processing Workshop, Online, Nov. 2020, pp. 85–93. doi: 10.18653/v1/2020.clinicalnlp-1.10.\n[5] V. Yogarajan, J. Montiel, T. Smith, and B. Pfahringer, ‘Transformers for Multi-label Classification of Medical Text: An Empirical Comparison’, in Artificial Intelligence in Medicine, vol. 12721, A. Tucker, P. Henriques Abreu, J. Cardoso, P. Pereira Rodrigues, and D. Riaño, Eds. Cham: Springer International Publishing, 2021, pp. 114–123. doi: 10.1007/978-3-030-77211-6_12.\n[6] H. Lu, L. Ehwerhemuepha, and C. Rakovski, ‘A comparative study on deep learning models for text classification of unstructured medical notes with various levels of class imbalance’, BMC Medical Research Methodology, vol. 22, no. 1, p. 181, Jul. 2022, doi: 10.1186/s12874-022-01665-y.\n[7] S. Casola, I. Lauriola, and A. Lavelli, ‘Pre-trained transformers: an empirical comparison’, Machine Learning with Applications, vol. 9, p. 100334, Sep. 2022, doi: 10.1016/j.mlwa.2022.100334.\n[8] A. K. B. Singh, M. Guntu, A. R. Bhimireddy, J. W. Gichoya, and S. Purkayastha, ‘Multi-label natural language processing to identify diagnosis and procedure codes from MIMIC-III inpatient notes’.\n[9] A. Haghighian Roudsari, J. Afshar, W. Lee, and S. Lee, ‘PatentNet: multi-label classification of patent documents using deep learning based language understanding’, Scientometrics, vol. 127, no. 1, pp. 207–231, Jan. 2022, doi: 10.1007/s11192-021-04179-4.\n[10] V. Yogarajan, J. Montiel, T. Smith, and B. Pfahringer, ‘Seeing The Whole Patient: Using Multi-Label Medical Text Classification Techniques to Enhance Predictions of Medical Codes’. arXiv, Mar. 28, 2020. Accessed: Jan. 30, 2023. [Online]. Available: http://arxiv.org/abs/2004.00430"
  },
  {
    "objectID": "pxtextmining/index.html",
    "href": "pxtextmining/index.html",
    "title": "Machine learning (pxtextmining)",
    "section": "",
    "text": "pxtextmining is a package built using Python. The package creates and trains models for categorising patient comments using the labels from the qualitative framework. The code is fully open source and is available on GitHub.\nOn a basic level, this is what is happening:\n\nThe dataset created using the qualitative framework is processed. This involves standardising capital letters to lowercase, or removing punctuation and numbers. The words are then converted into numbers, this process is called vectorisation. This is a necessary step to enable the models to handle the words.\nThe processed dataset is split into two parts, a training and a test set, usually in a ratio of about 3:1. The model is then trained on the training set. The test set is set aside completely and only used at the end to evaluate the performance of the trained model.\nDifferent model architectures and algorithms will be tried, each attempting to learn patterns from the dataset. The models will also be fine-tuned to improve performance. A full discussion of the model types explored will be posted on this website.\nAfter rigorous testing and experimentation, the model that is able to predict labels for patient comments most effectively will be selected and saved. This model will be used to label any new text comments that it receives.\nThe best model will be connected to the experiencesdashboard frontend via an API."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html",
    "href": "pxtextmining/performance_metrics.html",
    "title": "Performance metrics",
    "section": "",
    "text": "When we code machine learning models, we need to be able to measure how well they are performing. Performance metrics are the ‘scoring systems’ that we use to measure a model’s predictions. We keep a subset of the data aside that the model has never seen when training, and compare the model’s predicted labels with the real labels for that data. This is called the ‘test set’.\nWe would choose different performance metrics depending on our aims and objectives for the model. For this project, we are classifying patient comments, trying to label them by topic. Each comment can be referred to as a ‘sample’. A single-label classification model would assign each sample, or patient comment, to only one corresponding topic. In single-label classification, the most commonly used performance metrics are: accuracy, recall, precision, and f1 score."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#common-terminology-in-classification-models",
    "href": "pxtextmining/performance_metrics.html#common-terminology-in-classification-models",
    "title": "Performance metrics",
    "section": "Common terminology in classification models",
    "text": "Common terminology in classification models\nThe key terminology to understand is the concept of true positives, true negatives, false positives, and false negatives.\nTrue positive is when the sample has the label, and the model correctly predicts this label. An example is given in Table 1.\n\n\nTable 1: True positive and True negative example\n\n\n\n\n\n\n\nComment\nLabels given by qualitative analyst\nLabels given by hypothetical model\n\n\n\n\n“I really enjoyed dinner”\nFood & diet\nFood & diet\n\n\n\n\nIn this example, the model has also correctly predicted that the comment is NOT any other label, such as Medication. The comment in question has not been tagged as Medication, and the model was able to correctly identify this, predicting only Food & diet as the label instead. Because it predicted a negative for the label Medication for the comment ‘I really enjoyed dinner’, and this is the same as the real labelling for the comment, this is called a true negative for that label.\nIn contrast with true positives and true negatives, false positive and false negative are when the model has made a mistake. For the example in Table 2, the model has labelled the comment with Medication, although this is not a real label for the data. This means that it has made a false positive prediction for the Medication label.\nIt has also missed that the real label is Food & diet, meaning that it has made a false negative prediction for the label Food & diet.\n\n\nTable 2: False positive and False negative example\n\n\n\n\n\n\n\nComment\nLabels given by qualitative analyst\nLabels given by hypothetical model\n\n\n\n\n“I really enjoyed dinner”\nFood & diet\nMedication\n\n\n\n\nThe total number of true positives, true negatives, false positives and false negatives are usually put together in a table called a confusion matrix."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#single-label-performance-metrics",
    "href": "pxtextmining/performance_metrics.html#single-label-performance-metrics",
    "title": "Performance metrics",
    "section": "Single label performance metrics",
    "text": "Single label performance metrics\n\nAccuracy\nAccuracy is usually the concept that is most simple to grasp. It is the total number of correct predictions divided by the sum of the overall number of predictions. Range is between 0 to 1, the closer to 1 the better.\nHowever, it’s not always the best metric to use, particularly with imbalanced datasets. In Table 3 below, the model mostly predicts negatives. Of the 10 real positive values it is only able to predict this correctly 1 time. This means that the accuracy score is high (0.91), but it’s not very good at capturing the positive class when it does occur.\n\n\nTable 3: High accuracy is not always good!\n\n\n\nPredicted Negative\nPredicted Positive\n\n\n\n\nActual Negative\nTrue Negatives: 90\nFalse Positives: 0\n\n\nActual Positive\nFalse Negatives: 9\nTrue Positives: 1\n\n\n\n\n\n\nRecall\nRecall measures the ability of the model to detect occurrences of a class. For our example above, the model which had an accuracy of 0.91 only has a recall of 0.1. Recall is best used when it is important to identify as many occurrences of a class as possible, reducing false negatives but potentially increasing false positives. Range is between 0 to 1, the higher the better. The mathematical formula for calculating recall is:\nTrue Positives / (True Positives + False Negatives)\nI like to use the analogy of ‘recall’ as trying to optimise fishing by using a net. You are happy to get false positives (e.g. stones, debris) in your catch so that you can capture more fish overall.\nScenarios where a model might be optimised for recall include where you are trying to predict fraudulent bank transactions. It might be safer to flag more transactions as fraudulent (increasing potential false positives) and check them, in order to avoid false negatives.\nIn the context of pxtextmining, a model that is optimised for recall would perhaps assign more ‘false positives’. So you would be likely to get lots of labels which might not actually be applicable for a particular text. For a comment like ‘I really enjoyed dinner’, you might get the real label Food & diet, but also incorrect false positive labels such as Medication and Communication as well. The model will be more indiscriminate when assigning labels, so as not to accidentally miss a label.\n\n\nPrecision\nPrecision measures the ability of a model to avoid false alarms for a class, or the confidence of a model when predicting a specific class. It’s best used when it is important to be correct when identifying a class, reducing false positives but potentially increasing false negatives. Range is between 0 to 1, the higher the better. For the example in Table 3 the precision would be 1:\nTrue Positives / (True Positives + False Positives) = 1 / (1 + 0) = 1\nThe model did not identify any false positives at all, although it did have 9 false negatives, missing 9/10 of the target of interest. As we know, the recall was poor (0.1).\nThe model with high precision but low recall is very good at avoiding false positives, but unfortunately it also made a lot of false negative predictions. The analogy I like to use for understanding optimising for precision is that it is like trying to fish with a spear. You want to be sure that when you catch something, it is definitely a fish. You’re trying to minimise false positives, or anything that is not a fish, but you may miss some fish in doing so (maybe increasing false negatives).\nScenarios where a model might be optimised for precision include where you are trying to predict the safety of seatbelts. It is important to avoid false positives in this scenario. The cost of throwing away a potentially good seatbelt is relatively low, compared to the cost of equipping a car with a faulty seatbelt.\nIn the context of pxtextmining, a model that is optimised for precision may try to be more certain about a label before predicting it for a comment. So for a comment like ‘I really enjoyed dinner and the ward was comfortable’, which has the real labels Food & diet and Environment, the model may only be very certain that it is about Food & diet and label it as such. This means that it might miss the Environment label. This model would therefore produce more false negatives.\nPrecision and recall cannot both be increased at the same time, there is a tradeoff to be made between the two.\nStill confused? This video may help.\n\n\nF1 score\nF1 score is sometimes known as the harmonic mean of recall and precision. It’s an attempt to generalise the two. The range for this is between 0 to 1, the higher the better.\nIn phase 1 of the pxtextmining project, the metric that was optimised was class balance accuracy (not to be confused with balanced accuracy score).[1] This was a type of averaged accuracy score obtained across the different classes."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#multilabel-performance-metrics",
    "href": "pxtextmining/performance_metrics.html#multilabel-performance-metrics",
    "title": "Performance metrics",
    "section": "Multilabel performance metrics",
    "text": "Multilabel performance metrics\nIn a multilabel model, one sample can have more than one label. The exact number of labels assigned to the sample can vary. For example, we could label films like this:\n\n\n\nFilm\nLabels\n\n\n\n\nThe Mummy\nAction Adventure Comedy\n\n\nShrek\nComedy Animation\n\n\n\nPerformance measurement for models that perform multilabel classification is tricky. There is little consensus in the literature about which metric to select. This is because, as with the single-label example, it largely depends on what we’re trying to accomplish. The performance of the multi-label learning algorithm should therefore be tested on a broad range of metrics instead of only on the one being optimized.[2], [3] The current popular methods for reporting the performance of a multi-label classifier include the Hamming loss, precision, recall, and F-score for each class with the micro, macro, and weighted average of these scores from all classes.[4]\n\n\nTable 4: Example of multilabel scenario\n\n\nFilm\nReal Labels\nPredicted Labels\n\n\n\n\nThe Mummy\nAction Adventure Comedy\nAction Adventure\n\n\nShrek\nComedy Animation\nComedy Animation\n\n\n\n\n\nHamming loss\nThe Hamming loss is the fraction of labels that are incorrectly predicted. This ranges between 0 and 1, the lower the better. In the example in Table 4 above, there would be a Hamming loss of 0.125 as it only didn’t manage to capture 1 of the labels.\n\n\nJaccard score\nJaccard similarity index is the size of the intersection of the predicted labels and the true labels divided by the size of the union of the predicted and true labels. It ranges from 0 to 1, and 1 is the perfect score. However, if there are no true or predicted labels, the sklearn implementation of Jaccard will return an error.\n\n\nAveraging single label metrics\nAll of the usual classification metrics (Recall, Precision, F1 score) can be used in multilabel. For example, you could get a recall, precision, and F1 score for every label, on a one-vs-rest approach.\nIn multilabel, accuracy is sometimes also called ‘exact match accuracy’ or ‘subset accuracy’. This is quite a harsh metric, as all of the labels for the sample must be correct. In the example in Table 4, there would be a subset accuracy of only 0.5 as the model did not accurately predict all of the correct labels for the first sample, although it got 5 out of 6 labels right.\nYou could also obtain generalised recall, precision, and F1 scores for all the labels. The way that this works is the recall, precision, and F1 scores are obtained for each class, and then they are averaged out. They can be averaged in three ways:\n\nMicro-averaged: all samples equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated differently according to the total true positives, false negatives and false positives).\nMacro-averaged: all classes equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated equally in calculating the average)\nWeighted-averaged: each classes’ contribution to the average is weighted by support (the number of true instances for each label)."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#conclusion-which-metrics-to-use-for-pxtextmining-phase-2",
    "href": "pxtextmining/performance_metrics.html#conclusion-which-metrics-to-use-for-pxtextmining-phase-2",
    "title": "Performance metrics",
    "section": "Conclusion: which metrics to use for pxtextmining Phase 2?",
    "text": "Conclusion: which metrics to use for pxtextmining Phase 2?\nUltimately there is not going to be one magic metric that is going to work for the pxtextmining project. As the dataset is quite imbalanced, and we do want to capture the less well-represented classes, the macro-averaged scores are likely to be helpful. I expect to use the macro-averaged F1 score as a very rough indication in the beginning stages of the project.\nHowever, per-class statistics will likely be most important for performance measurement and especially in the model tuning stage will be helpful for indicating labels that are proving hard to capture.[5] The Hamming and Jaccard scores will also be calculated to give an overall indicator of performance although I anticipate that these will not be the primary focus of model tuning. Unfortunately the metric ‘class balance accuracy’ which was utilised in phase 1 of the pxtextmining project cannot be used for the multilabel models in phase 2.\nDo note that there are other metrics available. The ones selected here are the ones that have been most commonly mentioned in the literature, as it makes sense to utilise what has been established as best practice elsewhere for generalisability and interpretability."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#a-note-on-baselinedummy-models",
    "href": "pxtextmining/performance_metrics.html#a-note-on-baselinedummy-models",
    "title": "Performance metrics",
    "section": "A note on baseline/dummy models",
    "text": "A note on baseline/dummy models\nWhen prototyping machine learning models, we need to first establish a baseline of performance - what is the most basic level that we should expect from our model? In the case of an imbalanced dataset, a model which just predicts the mode (the most frequently occurring class) regardless of the input would be a suitable dummy. Other approaches to ‘dummy’ models include a model that just outputs random predictions.\nIt is useful to know what performance a dummy model would be able to achieve, when considering the performance of any machine learning model. This gives us something for us to compare our outputs to."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#bibliography",
    "href": "pxtextmining/performance_metrics.html#bibliography",
    "title": "Performance metrics",
    "section": "Bibliography",
    "text": "Bibliography\n\nL. Mosley, ‘A balanced approach to the multi-class imbalance problem’, Doctor of Philosophy, Iowa State University, Digital Repository, Ames, 2013. doi: 10.31274/etd-180810-3375.\nM.-L. Zhang and Z.-H. Zhou, ‘A Review on Multi-Label Learning Algorithms’, IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819–1837, Aug. 2014, doi: 10.1109/TKDE.2013.39.\nX.-Z. Wu and Z.-H. Zhou, ‘A Unified View of Multi-Label Performance Measures’, in Proceedings of the 34th International Conference on Machine Learning, Jul. 2017, pp. 3780–3788. Accessed: Jan. 20, 2023. [Online]. Available: https://proceedings.mlr.press/v70/wu17a.html\nM. Heydarian, T. E. Doyle, and R. Samavi, ‘MLCM: Multi-Label Confusion Matrix’, IEEE Access, vol. 10, pp. 19083–19095, 2022, doi: 10.1109/ACCESS.2022.3151048.\nS. Henning, W. H. Beluch, A. Fraser, and A. Friedrich, ‘A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing’. arXiv, Oct. 10, 2022. Accessed: Jan. 20, 2023. [Online]. Available: http://arxiv.org/abs/2210.04675"
  },
  {
    "objectID": "framework/framework2.html",
    "href": "framework/framework2.html",
    "title": "Framework development",
    "section": "",
    "text": "The inclusion of each sub-category in the framework is based on a robust evidence-based rationale, drawing on multiple strands of work, including research with patient experience and quality improvement teams across different types of 19 NHS Trusts and exploration of pre-existing frameworks including the one used in the phase 1 of this project. Crucially following on from this research the framework was built using a ground-up qualitative inductive coding approach. This consisted of a qualitative analyst manually reading a large volume of patient experience qualitative data and devising categories as they emerged in the data. This approach has meant that the sub-categories are:\n\nAll seen in patient experience qualitative data\nReflective of the issues patients themselves raise, how they present their experiences and the language they use (rather than attempting to fit experiences into pre-defined categories which would be a deductive rather than an inductive approach)\nConsidered to be distinct enough to form a unique sub-category in terms of vocabulary used and emphasis, thereby aiding their viability for an automated model\nCollectively able to result in an overall framework which makes comprehensive use of the data"
  },
  {
    "objectID": "framework/framework2.html#how-the-framework-was-built",
    "href": "framework/framework2.html#how-the-framework-was-built",
    "title": "Framework development",
    "section": "",
    "text": "The inclusion of each sub-category in the framework is based on a robust evidence-based rationale, drawing on multiple strands of work, including research with patient experience and quality improvement teams across different types of 19 NHS Trusts and exploration of pre-existing frameworks including the one used in the phase 1 of this project. Crucially following on from this research the framework was built using a ground-up qualitative inductive coding approach. This consisted of a qualitative analyst manually reading a large volume of patient experience qualitative data and devising categories as they emerged in the data. This approach has meant that the sub-categories are:\n\nAll seen in patient experience qualitative data\nReflective of the issues patients themselves raise, how they present their experiences and the language they use (rather than attempting to fit experiences into pre-defined categories which would be a deductive rather than an inductive approach)\nConsidered to be distinct enough to form a unique sub-category in terms of vocabulary used and emphasis, thereby aiding their viability for an automated model\nCollectively able to result in an overall framework which makes comprehensive use of the data"
  },
  {
    "objectID": "framework/framework2.html#further-development-to-the-framework",
    "href": "framework/framework2.html#further-development-to-the-framework",
    "title": "Framework development",
    "section": "Further development to the framework",
    "text": "Further development to the framework\nA large volume of qualitative data from across different trust types is being manually labelled by qualitative analysts (i.e., being assigned to the relevant sub-categories), to produce the dataset that will be used to train and test the machine learning model.\nAs this labelling occurs and quality assurance of the labelling is carried out the qualitative analysts will be vigilant for any emerging topics which have not already been seen in the data so new sub-categories can be added as required. Small amendments may also be made to the existing sub-category names and/or descriptions to ensure these remain well aligned to the qualitative data and reflect any nuances seen across different trust types.\nAs the work progresses decisions may need to be made around whether it makes sense to merge any of the sub-categories, particularly if they are low in volume as this will impact how well the model is able to identify them. Similarly, before the framework is finalised changes may be made to the category names, along with which sub-categories sit under each category. This will be guided by what is seen in the data and input from partner organisations."
  },
  {
    "objectID": "framework/index.html",
    "href": "framework/index.html",
    "title": "Qualitative framework",
    "section": "",
    "text": "The framework consists of the categories and subcategories that the automated model with assign the qualitative data to. The framework has multiple categories, each with their own set of sub-categories. The current framework is displayed below:\n\n\n\nQualitative framework as of March 2023\n\n\nWhen thinking about the categories and sub-categories, the categories can be thought of as the overarching heading grouping similar topics together in a meaningful way to help end users navigate the framework more easily. The sub-categories are more specific and better reflect the underlying data.\nEach sub-category has a description of what the data mapped there covers, for example:\n\n\n\nFurther detail on the ‘Number & deployment of staff’ subcategory\n\n\nHere are some examples of qualitative data that would map to the ‘Number & deployment of staff’ sub-category described above:\n\n‘Lots of staff’\n\n\n‘Staff shortages mean structured day activities can be cancelled at short notice & there is too much time wasted hanging around waiting to see if they run or not.’\n\n\n‘MORE STAFF PLEASE!!!!!!’\n\n\n‘Have less managers doing same job.’\n\n\n‘Staffing levels could improve. More recruitment, nursing assistants especially.’\n\n\n‘Would be good to have cover for support in situations where staff are off for extended periods due to holidays and sickness.’\n\n\n‘Don’t send trainee staff to people in crisis’\n\n\n‘The ward should only have female staff. Staff recruitment spoils what the ward does well.’"
  },
  {
    "objectID": "updates.html",
    "href": "updates.html",
    "title": "Updates",
    "section": "",
    "text": "Machine learning update\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nYiWen Hon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dashboard/index.html",
    "href": "dashboard/index.html",
    "title": "Dashboard",
    "section": "",
    "text": "The Experience dashboard is the front end tool for the PatientExperience-QDC project. It ties the back-end, the Pxtextmining, to the data source via an API and present metrics and graphs to help clinical staffs and managers quickly gain insight from patient experience data.\nThe dashboard automates the labeling/categorization of new data and gives users the functionality to interact with the data, do boolean searching on the text data, compare relationships across topics, visualize trend in the data, download reports, etc."
  },
  {
    "objectID": "dashboard/index.html#introduction",
    "href": "dashboard/index.html#introduction",
    "title": "Dashboard",
    "section": "",
    "text": "The Experience dashboard is the front end tool for the PatientExperience-QDC project. It ties the back-end, the Pxtextmining, to the data source via an API and present metrics and graphs to help clinical staffs and managers quickly gain insight from patient experience data.\nThe dashboard automates the labeling/categorization of new data and gives users the functionality to interact with the data, do boolean searching on the text data, compare relationships across topics, visualize trend in the data, download reports, etc."
  },
  {
    "objectID": "dashboard/index.html#the-structure",
    "href": "dashboard/index.html#the-structure",
    "title": "Dashboard",
    "section": "The structure",
    "text": "The structure\nThe dashboard is built using Shiny, an R package that makes it easy to build interactive web apps straight from R. The dashboard saves and load data from a database and gives users the ability to upload their unlabeled comment data in several formats for labeling. The dashboard processes feedback data and present several insight from the data using interactive tables, charts and other data format for easy digestion of the data. Figure 1 below shows the schematic diagram of the dashboard’s structure.\n\n\n\nFig 1. Dashboard structure"
  },
  {
    "objectID": "dashboard/index.html#planned-developments",
    "href": "dashboard/index.html#planned-developments",
    "title": "Dashboard",
    "section": "Planned developments",
    "text": "Planned developments\nBelow are the key features of the dashboard. We have a live demo of the experience dashboard here and the source code is here. Figure 2 shows a screenshot of the sample dashboard design…\n\n\n\nFig 2. Sample dashboard design\n\n\nSome of the features listed below are already implemented in the demo while some are in planned developments, e.g. boolean searching in text search\n\nData selection\nFrom the sidebar section of the dashboard, Users can drill down using the hierarchy level within their data such as divisions, directorate and teams and they can also filter by dates. The dashboard features and visualization changes in line with the selection.\n\n\nData upload and management section\nIn this tab, users have the functionality to\n\nAssign label to new data: Users can upload their data and once this data is uploaded the pxtextmining API is called to assign tags to the comments and the data is saved in the database.\nManage Data: users have the functionality to edit and delete rows within their data right from the dashboard environment.\n\n\n\nReport builder\nThere are four options to select and each adds sections to the report.\n\n% categories table: adds a table for the number of comments assigned to each category and percentage contributions of each category/topic\nVerbatim comments: the actual comments from the selected data\nSample demographics: the demographic graphs from the selected data\nFFT graph: the FFT plot\n\n\n\nFFT\nThis shows the Statistical Process Control (SPC) chart of the FFT score. The SPC chart is simply a line graph showing the FFT score in a chronological order, with the score on the vertical (y) axis, date on the horizontal (x) axis and the average score is shown as the centre line.\n\n\n\nFig 3. SPC chart of the FFT score\n\n\n\n\nTopics/weighting\nThis shows a table and plot for the number of comments in each category and users can select a category from the table or plot and all the comments relating to that category are displayed.\n\n\nComment search\nUsers can make a boolen search for comments with specific keyword(s).\n\n\nOverlap/Trend\nTrend in category: shows the monthly percentage contribution of each category\nOverlapping words: This shows which categories occur together more/less often based on the count or correlation of comments frequencies within the categories. Users will also be able to interact with the plot to see the comments any two categories have in common.\n\nDemographic tab\nTwo types of charts are shown here for each demographic variable\n\nDistribution of demographics in the sample.\nAverage percentage of maximum FFT score for each category."
  },
  {
    "objectID": "dashboard/dashboard2.html",
    "href": "dashboard/dashboard2.html",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "",
    "text": "This page shows a sample report of some of the additional analysis and graphs that will be available to download for users of the experience dashboard. This additional analysis might be much more useful for key staff working with the patient experience data and it aims to provide a more in-depth analysis of the comments so users can gain additional insights.\nPlease note that the data use for this report has been modified and the categories have been randomly assigned only for demonstration purpose. The aim of this sample report is to give you a feel of what will be delivered and not for conclusion to be drawn.\nThese visuals can be seen as additional data exploration tools to facilitate and lead to more detailed qualitative analysis of the categorised data, rather than replacing the need to read the underlying comments."
  },
  {
    "objectID": "dashboard/dashboard2.html#introduction",
    "href": "dashboard/dashboard2.html#introduction",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "",
    "text": "This page shows a sample report of some of the additional analysis and graphs that will be available to download for users of the experience dashboard. This additional analysis might be much more useful for key staff working with the patient experience data and it aims to provide a more in-depth analysis of the comments so users can gain additional insights.\nPlease note that the data use for this report has been modified and the categories have been randomly assigned only for demonstration purpose. The aim of this sample report is to give you a feel of what will be delivered and not for conclusion to be drawn.\nThese visuals can be seen as additional data exploration tools to facilitate and lead to more detailed qualitative analysis of the categorised data, rather than replacing the need to read the underlying comments."
  },
  {
    "objectID": "dashboard/dashboard2.html#overlapping-words",
    "href": "dashboard/dashboard2.html#overlapping-words",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "1. Overlapping words",
    "text": "1. Overlapping words\nThis shows relationships between the topics (the categories). It shows the categories that tend to be similar to each other in how they are assigned to comments i.e. how comments shared about one topic relates to another topic. These relationships are shown using the number of comments shared between the topics (comment frequencies) or the Pearson correlation of the comment frequencies. Of particular interest is the correlation among topics, which indicates how often they are assigned together to comments relative to how often they are assigned separately.\n\n\n\n\n\n\n\n\nWe used a Network graph and heatmap for this purpose. In this sample report, the Network graph is used to show the relationship based on the pairwise correlation and the heatmap shows the actual comment frequencies. We can see how the Staff topic shows a strong relationship to the Communication & involvement topic, this indicates comments relating to Staff often relate more to the Communication & involvement category compared to other categories.\nNOTE: The heatmap is available on the dashboard and users will be able to add the network graph to their report (available to download from the dashboard) if they find it more intuitive."
  },
  {
    "objectID": "dashboard/dashboard2.html#prominent-words-unique-to-each-category",
    "href": "dashboard/dashboard2.html#prominent-words-unique-to-each-category",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "2. Prominent Words unique to each Category",
    "text": "2. Prominent Words unique to each Category\nWe’d expect the categories to differ in terms of topic and content, and therefore the frequency of words should differ between them also. The TF-IDF (term frequency-inverse document frequency) is used to quantify the relevance of words or phrases in a document amongst a collection of documents. Here we regard each topic/category as a document and we used the TF-IDF to find the prominent words peculiar to each of the categories. Below, we visualized the top 5 prominent words from each category. Note that some categories have less than 5 words. The Travel & Transport and Service location have same prominent words because of the high correlation between both categories as can be seen in the topic correlation network graph above."
  },
  {
    "objectID": "dashboard/dashboard2.html#word-co-occurence-across-categories",
    "href": "dashboard/dashboard2.html#word-co-occurence-across-categories",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "3. Word co-occurence across categories",
    "text": "3. Word co-occurence across categories\n\n\n\n\n\nThe Network graph above shows the top 5 words used in each category. The words friendly and staff have high present in comments labelled ‘Staff’. We also see how the “Mental Health Specifics” and “Medication” categories stand out compared to the linkage we see among other groups.\n\n\n\n\n\nThe above graph shows how the top-occurring words within all the comments combined are distributed across the categories. We can see that among the top 20 frequent words in all the comments combined, most occurred within comments tagged “Care received”. The words staff, helpful and feel account for most of the linkage between “Staff”, “Communication and Involvement” and “General” categories"
  },
  {
    "objectID": "dashboard/dashboard2.html#word-to-word-connections",
    "href": "dashboard/dashboard2.html#word-to-word-connections",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "4. Word-to-word connections",
    "text": "4. Word-to-word connections\n\n\n\n\n\nThe above network graph shows the directional linkage between the most common bigrams (pairs of consecutive words) in the whole data. It shows bigrams that occurred more than 5 times and where neither word was a stop word. The transparency of the arrows between the words is based on how common or rare the bigram is in the data.\nFriendly formed a major node and we see common short phrases such as Mental Health, friendly staff, and feel comfortable"
  },
  {
    "objectID": "dashboard/dashboard2.html#highlight-complex-comments",
    "href": "dashboard/dashboard2.html#highlight-complex-comments",
    "title": "Additional Analysis for Experiences Dashboard",
    "section": "5. Highlight complex comments",
    "text": "5. Highlight complex comments\nWe identify there may be situations where the machine learning model might be wrong in the categories assigned to comments, and this may be especially true for very long comments. We set some criteria to catch those complex comments for further interrogation. Below is a sample criterion to highlight complex comments (if any are met):\n\ncomments with over 50 words\ncomments with over 3 label attached\n\nIn this sample data, there is a total of 36 comments out of a total of 1998 comments that might need additional attention.\nThese complex comments will be available to download from the dashboard for users to further interrogate the comments and the assigned categories."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PatientExperience-QDC website",
    "section": "",
    "text": "This is the website for the NHS England (NHSE) Insight and Feedback Team funded Patient Experience Qualitative Data Categorisation (PatientExperience-QDC) project. This project is being conducted in partnership with the Nottinghamshire Healthcare NHS Foundation Trust Clinical Development Unit Data Science Team.\nThe aim of this project is develop and test an innovative approach to free text analysis, to support better use of qualitative patient experience feedback gathered through the NHS Friends and Family Test.\nThere are three main elements to the project:\n\nCreation of a dataset of free text patient feedback that has been labelled with topics, utilising a new categorisation framework which has been developed by qualitative analysts at NHSE.\nPxtextmining: Development and training of machine learning models to automatically categorise unlabelled patient feedback into the categories established in the categorisation framework.\nExperiences Dashboard: Development of a user friendly interface to enable the exploration of qualitative patient feedback."
  },
  {
    "objectID": "index.html#patient-experience-qualitative-data-categorisation",
    "href": "index.html#patient-experience-qualitative-data-categorisation",
    "title": "PatientExperience-QDC website",
    "section": "",
    "text": "This is the website for the NHS England (NHSE) Insight and Feedback Team funded Patient Experience Qualitative Data Categorisation (PatientExperience-QDC) project. This project is being conducted in partnership with the Nottinghamshire Healthcare NHS Foundation Trust Clinical Development Unit Data Science Team.\nThe aim of this project is develop and test an innovative approach to free text analysis, to support better use of qualitative patient experience feedback gathered through the NHS Friends and Family Test.\nThere are three main elements to the project:\n\nCreation of a dataset of free text patient feedback that has been labelled with topics, utilising a new categorisation framework which has been developed by qualitative analysts at NHSE.\nPxtextmining: Development and training of machine learning models to automatically categorise unlabelled patient feedback into the categories established in the categorisation framework.\nExperiences Dashboard: Development of a user friendly interface to enable the exploration of qualitative patient feedback."
  },
  {
    "objectID": "project_team.html",
    "href": "project_team.html",
    "title": "Project team",
    "section": "",
    "text": "Chris Beeley, Senior Lead Data Scientist, The Strategy Unit\nEstelle Phillips, Qualitative Senior Analyst, NHS England\nOluwasegun Apejoye, Shiny Developer, Nottinghamshire Healthcare NHS Foundation Trust\nPete Williamson, Insight and Feedback Manager, NHS England\nYiWen Hon, Data Scientist, Nottinghamshire Healthcare NHS Foundation Trust"
  },
  {
    "objectID": "posts/update-23-03-13/index.html",
    "href": "posts/update-23-03-13/index.html",
    "title": "Machine learning update",
    "section": "",
    "text": "We’ve had a chance to play around with a few different model architectures now and the main finding is that so far, as in Phase 1 and other similar projects, Support Vector Classifier is performing the best. We are currently prototyping using only some of the final dataset, and focusing on predicting 1 of 13 overarching ‘major categories’. These are divided into a further 50+ subcategories, which our models are not yet trained to detect, but this will be the next step, once the framework is finalised.\nWe have also tried out Distilbert uncased, which is performing well, if not slightly better, than SVC. However, it’s taking a long time to train (around 6 hours on average) and is very large, around 1 GB in size. It also takes a long time to make predictions, around 1 minute per 1000 comments. When we have the final dataset we’ll have a clearer picture of which model will be best suited for the needs of the project.\nThe latest development is that we’re able to combine the text with the ‘question type’ feature. Most of the questions asked by trusts can be divided into one of three main types:\n\nWhat did we do well?\nWhat could we do better?\nAny other comments / no specific question\n\nIncluding the question type with the text of the comment has improved performance slightly. The way this has been done with BERT can be seen in the diagram below. We tried both of the methods found in this stackoverflow post, which produced similar results. We went for the concatenated layers, however, as this means the model architecture is more flexible and can be adapted to include other features as well, if necessary.\n\n\n\nDistilbert model with categorical question type feature"
  },
  {
    "objectID": "background/phase_1.html",
    "href": "background/phase_1.html",
    "title": "Phase 1 overview",
    "section": "",
    "text": "A short summary of the report for phase 1 of the project is available on this page. The full report can be downloaded here\nPhase 1 of the text mining pilot project hosted at Nottinghamshire Healthcare NHS Foundation Trust and funded by NHS England took place between 2021-22. This formed the basis of the current Patient Experience-QDC project. This project aimed to improve the use of FFT and patient experience survey qualitative data in selected trusts, working towards generating from this piece of work a national “support or guidance toolkit” to help drive service improvements. This project’s overarching goal was to create text mining software, originating in the NHS, which analyses qualitative patient experience data for theme and sentiment, displays this on an online dashboard and is freely available to all NHS Trusts.\nObjectives of phase 1:\nThe project was characterised by three distinct outputs:\nMachine learning model and dataset\nThe models that were trained in phase 1 of the project were trained on approximately 10,000 rows of data that were provided by three partner trusts, and coded using the Python scikit-learn library. There were two models trained that were able to predict two different targets:\nThe winning model for the nine themes was a linear support vector machine with 71% accuracy and 52% class balance accuracy on the test set. This is available on github, together with the code used to create the model and finetune the hyperparameters.\nThe winning model for the nine criticality values was a logistic regression with 59% accuracy and 44% class balance accuracy on the test set. This is also available on github, together with the code used to create the model and finetune the hyperparameters.\nDashboard\nThe two dashboards were created using Shiny/R. The model summary dashboard provided contextual information regarding the trained machine learning models. The other dashboard, experiencesdashboard, was one of the primary outputs of the project. Functionality available in this dashboard was:"
  },
  {
    "objectID": "background/phase_1.html#learning-and-feedback-from-phase-1",
    "href": "background/phase_1.html#learning-and-feedback-from-phase-1",
    "title": "Phase 1 overview",
    "section": "Learning and feedback from phase 1",
    "text": "Learning and feedback from phase 1\nTo evaluate phase 1 of the project, semi-structured interviews with three of the partner trusts were conducted. Participants were asked to consider whether or not the project outputs met objectives 1 and 3, in particular. The key findings were:\nMachine learning model\nOverall, participants in the evaluation found the algorithm useful, and felt that it could potentially save time and lead to better use of qualitative patient feedback. It was also seen as more sophisticated than the existing commercial solution utilised by one trust. Suggestions for improvements included:\n\nRevision of the themes/categories to better align with the needs of different trust types was suggested, as the phase 1 data was largely skewed towards community trusts.\nMore granular level of thematic tagging (subcategories) was suggested as being helpful as the categories in phase 1 were quite broad.\nPhase 1 had only 1 label for each comment, whereas all respondents stated that in their experience, many comments had multiple labels. The ability to detect multiple topics in a comment text was deemed important functionality in future developments of the algorithm.\nPerformance of criticality/sentiment labelling in phase 1 of the project was not deemed sufficiently reliable to inform decision making.\n\nDashboard \nThe dashboard from phase 1 was easy to use and the searching and presentation of information was commended. Areas for improvement included: More readable/understandable visualisations, through improvements to the design choices\n\nInclusion of trend data (showing changes over time) and other data visualisation\nAlternative output options e.g. PDF, Power BI\nA more usable/intuitive dashboard design\n\nProject management \nInterview participants felt that the project was overall successful in meeting its objectives, and expressed a desire to continue their involvement in further development of the solution. In particular, they praised:\n\nThe project structure, with its initiation and running from within the NHS\nThe knowledge and expertise of the project lead\n\nAreas for improvement included:\n\nClarity around the role of NHSE and increased input to help coordinate expertise\nMore structure and management for the project, as it was all centred around one individual (the project lead)\nBetter opportunity for participating partner trusts to interact with each other and to interrogate decisions and processes in the project\nInformation sharing, with a better balance between detailed technical information and non-technical explanations."
  },
  {
    "objectID": "background/background.html",
    "href": "background/background.html",
    "title": "Background",
    "section": "",
    "text": "NHS England commissions the Friends and Family Test (FFT), a continuous improvement tool allowing patients and people who use NHS services to feed back on their experience. The NHS constitution sets out a requirement to collect and act on patient feedback – the FFT supports that requirement and provides a framework to do it. Since 2013, NHS provider organisations have been mandated to collect feedback (including qualitative feedback in the form of written comments) via the FFT.\nPrevious work by the project team at Nottinghamshire Healthcare NHS Foundation Trust has provided an algorithm/system that can classify FFT qualitative data according to categories and how positive or negative they are, but the system has not been tested on the breadth of FFT data available and the system tags to only eight categories, which cannot be altered by users. This system is also trained on responses to one question, which could impact accuracy when different questions are used.\nThe Patient Experience Qualitative Data Categorisation (PatientExperience-QDC) project aims to create a minimal viable product (MVP) that can categorise FFT data according to defined user needs across different service settings (e.g. acute outpatients/inpatients, mental health, community, ambulance, children’s services).\nThe project will also explore and increase the granularity of the categories with which the model can categorise feedback and support adding multiple tags to comments where more than one category is present. To help ensure all categories are fit for purpose a framework for categorisation will be developed using a ground up approach, then carefully validated and checked through a through a thorough quality assurance process.\nA founding principle of this work is that the methodology and code should be completely open. Providing the methodology and code behind a solution is important both to allow expert users to satisfy themselves of the rigour of the methodology, as well as allowing anybody across the NHS to run this code and benefit from the system without any payment. Moreover, coding in the open is an important way to build collaborative links across the NHS and government generally."
  },
  {
    "objectID": "background/background.html#project-objectives-2022-23",
    "href": "background/background.html#project-objectives-2022-23",
    "title": "Background",
    "section": "PROJECT OBJECTIVES 2022-23",
    "text": "PROJECT OBJECTIVES 2022-23\nThe aims and objectives of this programme are, broadly, threefold. Firstly, to ensure any new categories and framework are fit for purpose through stakeholder engagement and thorough quality assurance processes. This will involve a significant data labelling exercise to produce the required training data across a breadth of service settings. Secondly, the project will explore and implement multi-tagging methods as a priority. Finally, a user-friendly interface will be generated, including user guidance, with input from users such as trust and other stakeholders. User guidance will incorporate guidance on how to use the categorised qualitative outputs, aligning with training materials that are being developed by NHS England to support qualitive analysis.\n\nEnsure all categories are fit for purpose - they must be validated (through user needs engagement and evaluation) and checked with input from providers and other stakeholders.\nCreate a MVP that can categorise FFT data according to defined user needs across different service settings.\nBuild on the machine learning approaches used in the previous work, where possible, increase the granularity of categories to which the model can assign feedback by adding subcategories.\nEnsure the solution can be used on FFT data, e.g. from different service settings and questions. Through pilot and partner organisations, test and confirm accuracy and explore usefulness in a range of service settings for FFT.\nExplore a range of methods to ensure feedback can be labelled with multiple categories (and sentiment or criticality), for example where one comment contains multiple topics. The project will also consider how data is labelled and presented, for example splitting comments into individual mentions.\nDevelop a user-friendly interface (dashboard), including guidance, that meets user needs, e.g. easy to use, timely display of data, reporting and analysis functionality, triangulation options and inclusion of demographics. This will include options for both on-premise and cloud platforms.\nEnsure that the work is open, reproducible, produced with free and open source components, and work nationally to promote use of the system and its components in order to increase good practice in this area\nCollect user case examples of deployment and use of the solution for service improvement.\nCreate easy to use guidance material for deployment and use of the software for both cloud and locally hosted solutions."
  },
  {
    "objectID": "background/background.html#partner-organisations",
    "href": "background/background.html#partner-organisations",
    "title": "Background",
    "section": "Partner organisations",
    "text": "Partner organisations\nPilot and Partner organisations (trusts) will be actively engaged with and involved in decision making to shape developments in the project. Organisations can decide/choose their level of commitment to the project. However, the aim is that partner organisations will provide FFT data to support training and validation of the algorithm, test/pilot the solution, and input to dashboard, framework, guidance material design. We are currently working with six organisations covering a range of different service settings, including settings including acute OP/IP, mental health, community, ambulance, A&E and paediatrics.\nIf you are interested in being a partner trust or would like to have a discussion about the project please contact the project team."
  },
  {
    "objectID": "background/background.html#rough-timeline",
    "href": "background/background.html#rough-timeline",
    "title": "Background",
    "section": "Rough timeline",
    "text": "Rough timeline\n\n\n\nTimeline for 2022-23"
  },
  {
    "objectID": "background/background.html#project-outputs",
    "href": "background/background.html#project-outputs",
    "title": "Background",
    "section": "Project outputs",
    "text": "Project outputs\nResults will be presented in the form of a report with key findings and guidance on how to deploy and implement the product locally. This will include, but is not limited to, the algorithm code and dashboard template, and implementation guidance for the developed software, so that the work can be shared and reproduced in other trusts/organisations as well as a best practice document that can be used as a toolkit to support trusts.\nFeedback will be collected from organisations during and beyond the project timeline to help inform future case studies and evaluate the solution to understand what is/isn’t working well.\nThe continuity of the solution beyond the end of the project is yet to be decided. However, the outputs from this project are open source and can be used by anyone."
  }
]