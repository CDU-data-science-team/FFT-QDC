[
  {
    "objectID": "dashboard/dashboard2.html",
    "href": "dashboard/dashboard2.html",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "",
    "text": "This page shows an additional analysis and graphs that will be available to download for users of the experience dashboard. This additional analysis might be much more useful for key staff working with the patient experience data and it aims to provide a more in-depth analysis of the comments so users can gain additional insights."
  },
  {
    "objectID": "dashboard/dashboard2.html#overlapping-words",
    "href": "dashboard/dashboard2.html#overlapping-words",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "1. Overlapping words",
    "text": "1. Overlapping words\nThis shows relationships between the themes (the categories). It shows the categories that tend to be similar to each other in how they are assigned to comments i.e. how comments shared about one theme relates to another theme. These relationships are shown using the number of comments shared between the themes (comment frequencies) or the Pearson correlation of the comment frequencies. Of particular interest is the correlation among themes, which indicates how often they are assigned together to comments relative to how often they are assigned separately.\n\n\n\n\n\n\n\n\nWe used a Network graph and heatmap for this purpose. In this sample report, the Network graph is used to show the relationship based on the pairwise correlation and the heatmap shows the actual comment frequencies. We can see how the Staff theme shows a strong relationship to the Communication & involvement theme, this indicates comments relating to Staff often relate more to the Communication & involvement category compared to other categories.\nNOTE: The heatmap is available on the dashboard and users will be able to add the network graph to their report (available to download from the dashboard) if they find it more intuitive."
  },
  {
    "objectID": "dashboard/dashboard2.html#tf-idfword-counts",
    "href": "dashboard/dashboard2.html#tf-idfword-counts",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "2. TF-IDF/Word counts",
    "text": "2. TF-IDF/Word counts\nWe’d expect the categories to differ in terms of topic and content, and therefore the frequency of words should differ between them also. The TF-IDF (term frequency-inverse document frequency) is used to quantify the relevance of words or phrases in a document amongst a collection of documents. Here we regard each theme/category as a document and we used the TF-IDF to find the prominent words peculiar to each of the categories. Below, we visualized the top 15 prominent words from each category."
  },
  {
    "objectID": "dashboard/dashboard2.html#word-co-occurence-across-categories",
    "href": "dashboard/dashboard2.html#word-co-occurence-across-categories",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "3. Word co-occurence across categories",
    "text": "3. Word co-occurence across categories\n\n\n\n\n\nThe Network graph above shows the top 5 words used in each category. The words friendly and staff have high present in comments labelled ‘Staff’. We also see how the “Mental Health Specifics” and “Medication” categories stand out compared to the linkage we see among other groups.\n\n\n\n\n\nThe above graph shows how the top-occurring words within all the comments combined are distributed across the categories. We can see that among the top 20 frequent words in all the comments combined, most occurred within comments tagged “Care received”. The words staff, helpful and feel account for most of the linkage between “Staff”, “Communication and Involvement” and “General” categories"
  },
  {
    "objectID": "dashboard/dashboard2.html#word-to-word-connections",
    "href": "dashboard/dashboard2.html#word-to-word-connections",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "4. Word-to-word connections",
    "text": "4. Word-to-word connections\n\n\n\n\n\nThe above network graph shows the directional linkage between the most common bigrams (pairs of consecutive words) in the whole data. It shows bigrams that occurred more than 5 times and where neither word was a stop word. The transparency of the arrows between the words is based on how common or rare the bigram is in the data.\nFriendly formed a major node and we see common short phrases such as Mental Health, friendly staff, and feel comfortable"
  },
  {
    "objectID": "dashboard/dashboard2.html#highlight-complex-comments",
    "href": "dashboard/dashboard2.html#highlight-complex-comments",
    "title": "Additional Analysis for ExperiencesDashboard",
    "section": "5. Highlight complex comments",
    "text": "5. Highlight complex comments\nWe identify there may be situations where the machine learning model might be wrong in the categories assigned to comments, and this may be especially true for very long comments. We set some criteria to catch those complex comments for further interrogation. Below is a sample criterion to highlight complex comments (if any are met):\n\ncomments with over 50 words\ncomments with over 3 label attached\n\nIn this sample data, there is a total of 36 comments that might need additional attention.\nThese complex comments will be available to download from the dashboard for users to further interrogate the comments and the assigned categories."
  },
  {
    "objectID": "dashboard/index.html",
    "href": "dashboard/index.html",
    "title": "Dashboard",
    "section": "",
    "text": "The Experiencedashboard is the front end tool for the FFT-QDC project. It ties the back-end, the Pxtextmining, to the data source via an API and present metrics and graphs to help clinical staffs and managers quickly gain insight from patient experience data.\nThe dashboard automates the labeling/categorization of new data and gives users the functionality to interact with the data, do boolean searching on the text data, compare relationships across themes, visualize trend in the data, download reports, etc."
  },
  {
    "objectID": "dashboard/index.html#the-structure",
    "href": "dashboard/index.html#the-structure",
    "title": "Dashboard",
    "section": "The structure",
    "text": "The structure\nThe dashboard is built using Shiny, an R package that makes it easy to build interactive web apps straight from R. The dashboard saves and load data from a database and gives users the ability to upload their unlabeled comment data in several formats for labeling. The dashboard processes feedback data and present several insight from the data using interactive tables, charts and other data format for easy digestion of the data. The figure below shows the schematic diagram of the dashboard’s structure."
  },
  {
    "objectID": "dashboard/index.html#planned-developments",
    "href": "dashboard/index.html#planned-developments",
    "title": "Dashboard",
    "section": "Planned developments",
    "text": "Planned developments\nBelow are the key features of the dashboard. We have a live demo of the experiencedashboard here and the source code is here. Some of the features listed below are already implemented in the demo while some are in planned developments, e.g. boolean searching in text search\n\nData selection\nFrom the sidebar section of the dashboard, Users can drill down using the hierarchy level within their data such as divisions, directorate and teams and they can also filter by dates. The dashboard features and visualization changes in line with the selection.\n\n\nData upload and management section\nIn this tab, users have the functionality to\n\nAssign label to new data: Users can upload their data and once this data is uploaded the pxtextmining API is called to assign tags to the comments and the data is saved in the database.\nManage Data: users have the functionality to edit and delete rows within their data right from the dashboard environment.\n\n\n\nReport builder\nThere are four options to select and each adds sections to the report.\n\n% categories table: adds a table for the number of comments assigned to each category and percentage contributions of each category/theme\nVerbatim comments: the actual comments from the selected data\nSample demographics: the demographic graphs from the selected data\nFFT graph: the FFT plot\n\n\n\nFFT\nThis shows the Statistical Process Control (SPC) chart of the FFT score\n\n\nThemes/ weighting\nShows a table and plot for the number of comments in each category and users can select a category from the table or plot and all the comments relating to that category are displayed\n\n\nComment search\nUsers can make a boolen search for comments with specific keyword(s)\n\n\nOverlap/Trend\nTrend in category: shows the monthly percentage contribution of each category\nOverlapping words: This shows which categories occur together more/less often based on the count or correlation of comments frequencies within the categories\n\nDemographic tab\nTwo types of charts are shown here for each demographic variable\n\nDistribution of demographics in the sample\nAverage percentage of maximum FFT score for each category"
  },
  {
    "objectID": "updates.html",
    "href": "updates.html",
    "title": "Updates",
    "section": "",
    "text": "FILLER\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nYiWen Hon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FFT-QDC website",
    "section": "",
    "text": "This website is still under development and is incomplete"
  },
  {
    "objectID": "index.html#friends-and-family-test-qualitative-data-categorisation",
    "href": "index.html#friends-and-family-test-qualitative-data-categorisation",
    "title": "FFT-QDC website",
    "section": "Friends and Family Test Qualitative Data Categorisation",
    "text": "Friends and Family Test Qualitative Data Categorisation\nThis is the website for the NHS England (NHSE) Insight and Feedback Team funded Friends and Family Test Qualitative Data Categorisation (FFT-QDC) project. This project is being conducted in partnership with the Nottinghamshire Healthcare NHS Foundation Trust Clinical Decision Unit Data Science Team.\nThe aim of this project is develop and test an innovative approach to free text analysis, to support better use of qualitative patient experience feedback gathered through the NHS Friends and Family Test.\nThere are three main elements to the project:\n\nCreation of a dataset of free text patient feedback that has been labelled with themes, utilising a new categorisation framework which has been developed by qualitative researchers at NHSE.\nPxtextmining: Development and training of machine learning models to automatically categorise unlabelled patient feedback into the categories established in the categorisation framework.\nExperiences Dashboard: Development of a user friendly interface to enable the exploration of qualitative patient feedback."
  },
  {
    "objectID": "framework/framework2.html",
    "href": "framework/framework2.html",
    "title": "framework2",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "framework/index.html",
    "href": "framework/index.html",
    "title": "Framework",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "posts/update-23-01-31/index.html",
    "href": "posts/update-23-01-31/index.html",
    "title": "FILLER",
    "section": "",
    "text": "Bla"
  },
  {
    "objectID": "pxtextmining/index.html",
    "href": "pxtextmining/index.html",
    "title": "Machine learning (pxtextmining)",
    "section": "",
    "text": "pxtextmining is a package built using Python. The package creates and trains models for categorising patient comments using the labels from the qualitative framework. The code is fully open source and is available on GitHub.\nOn a basic level, this is what is happening:\n\nThe dataset created using the qualitative framework is processed. This involves standardising capital letters to lowercase, or removing punctuation and numbers. The words are then converted into numbers, this process is called vectorisation. This is a necessary step to enable the models to handle the words.\nThe processed dataset is split into two parts, a training and a test set, usually in a ratio of about 3:1. The model is then trained on the training set. The test set is set aside completely and only used at the end to evaluate the performance of the trained model.\nDifferent model architectures and algorithms will be tried, each attempting to learn patterns from the dataset. The models will also be fine-tuned to improve performance. A full discussion of the model types explored will be posted on this website.\nAfter rigorous testing and experimentation, the model that is able to predict labels for patient comments most effectively will be selected and saved. This model will be used to label any new text comments that it receives.\nThe best model will be connected to the experiencesdashboard frontend via an API."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html",
    "href": "pxtextmining/performance_metrics.html",
    "title": "Performance metrics",
    "section": "",
    "text": "When we code machine learning models, we need to be able to measure how well they are performing. Performance metrics are the ‘scoring systems’ that we use to measure a model’s predictions. We keep a subset of the data aside that the model has never seen when training, and compare the model’s predicted labels with the real labels for that data. This is called the ‘test set’.\nWe would choose different performance metrics depending on our aims and objectives for the model. For this project, we are classifying patient comments, trying to label them by topic. Each comment can be referred to as a ‘sample’. A single-label classification model would assign each sample, or patient comment, to only one corresponding topic. In single-label classification, the most commonly used performance metrics are: accuracy, recall, precision, and f1 score."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#common-terminology-in-classification-models",
    "href": "pxtextmining/performance_metrics.html#common-terminology-in-classification-models",
    "title": "Performance metrics",
    "section": "Common terminology in classification models",
    "text": "Common terminology in classification models\nThe key terminology to understand is the concept of true positives, true negatives, false positives, and false negatives.\nTrue positive is when the sample has the label, and the model correctly predicts this label. An example is given in Table 1.\n\n\nTable 1: True positive and True negative example\n\n\n\n\n\n\n\nComment\nLabels given by qualitative researcher\nLabels given by hypothetical model\n\n\n\n\n“I really enjoyed dinner”\nFood & diet\nFood & diet\n\n\n\n\nIn this example, the model has also correctly predicted that the comment is NOT any other label, such as Medication. The comment in question has not been tagged as Medication, and the model was able to correctly identify this, predicting only Food & diet as the label instead. Because it predicted a negative for the label Medication for the comment ‘I really enjoyed dinner’, and this is the same as the real labelling for the comment, this is called a true negative for that label.\nIn contrast with true positives and true negatives, false positive and false negative are when the model has made a mistake. For the example in Table 2, the model has labelled the comment with Medication, although this is not a real label for the data. This means that it has made a false positive prediction for the Medication label.\nIt has also missed that the real label is Food & diet, meaning that it has made a false negative prediction for the label Food & diet.\n\n\nTable 2: False positive and False negative example\n\n\n\n\n\n\n\nComment\nLabels given by qualitative researcher\nLabels given by hypothetical model\n\n\n\n\n“I really enjoyed dinner”\nFood & diet\nMedication\n\n\n\n\nThe total number of true positives, true negatives, false positives and false negatives are usually put together in a table called a confusion matrix."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#single-label-performance-metrics",
    "href": "pxtextmining/performance_metrics.html#single-label-performance-metrics",
    "title": "Performance metrics",
    "section": "Single label performance metrics",
    "text": "Single label performance metrics\n\nAccuracy\nAccuracy is usually the concept that is most simple to grasp. It is the total number of correct predictions divided by the sum of the overall number of predictions. Range is between 0 to 1, the closer to 1 the better.\nHowever, it’s not always the best metric to use, particularly with imbalanced datasets. In Table 3 below, the model mostly predicts negatives. Of the 10 real positive values it is only able to predict this correctly 1 time. This means that the accuracy score is high (0.91), but it’s not very good at capturing the positive class when it does occur.\n\n\nTable 3: High accuracy is not always good!\n\n\n\nPredicted Negative\nPredicted Positive\n\n\n\n\nActual Negative\nTrue Negatives: 90\nFalse Positives: 0\n\n\nActual Positive\nFalse Negatives: 9\nTrue Positives: 1\n\n\n\n\n\n\nRecall\nRecall measures the ability of the model to detect occurrences of a class. For our example above, the model which had an accuracy of 0.91 only has a recall of 0.1. Recall is best used when it is important to identify as many occurrences of a class as possible, reducing false negatives but potentially increasing false positives. Range is between 0 to 1, the higher the better. The mathematical formula for calculating recall is:\nTrue Positives / (True Positives + False Negatives)\nI like to use the analogy of ‘recall’ as trying to optimise fishing by using a net. You are happy to get false positives (e.g. stones, debris) in your catch so that you can capture more fish overall.\nScenarios where a model might be optimised for recall include where you are trying to predict fraudulent bank transactions. It might be safer to flag more transactions as fraudulent (increasing potential false positives) and check them, in order to avoid false negatives.\nIn the context of pxtextmining, a model that is optimised for recall would perhaps assign more ‘false positives’. So you would be likely to get lots of labels which might not actually be applicable for a particular text. For a comment like ‘I really enjoyed dinner’, you might get the real label Food & diet, but also incorrect false positive labels such as Medication and Communication as well. The model will be more indiscriminate when assigning labels, so as not to accidentally miss a label.\n\n\nPrecision\nPrecision measures the ability of a model to avoid false alarms for a class, or the confidence of a model when predicting a specific class. It’s best used when it is important to be correct when identifying a class, reducing false positives but potentially increasing false negatives. Range is between 0 to 1, the higher the better. For the example in Table 3 the precision would be 1:\nTrue Positives / (True Positives + False Positives) = 1 / (1 + 0) = 1\nThe model did not identify any false positives at all, although it did have 9 false negatives, missing 9/10 of the target of interest. As we know, the recall was poor (0.1).\nThe model with high precision but low recall is very good at avoiding false positives, but unfortunately it also made a lot of false negative predictions. The analogy I like to use for understanding optimising for precision is that it is like trying to fish with a spear. You want to be sure that when you catch something, it is definitely a fish. You’re trying to minimise false positives, or anything that is not a fish, but you may miss some fish in doing so (maybe increasing false negatives).\nScenarios where a model might be optimised for precision include where you are trying to predict the safety of seatbelts. It is important to avoid false positives in this scenario. The cost of throwing away a potentially good seatbelt is relatively low, compared to the cost of equipping a car with a faulty seatbelt.\nIn the context of pxtextmining, a model that is optimised for precision may try to be more certain about a label before predicting it for a comment. So for a comment like ‘I really enjoyed dinner and the ward was comfortable’, which has the real labels Food & diet and Environment, the model may only be very certain that it is about Food & diet and label it as such. This means that it might miss the Environment label. This model would therefore produce more false negatives.\nPrecision and recall cannot both be increased at the same time, there is a tradeoff to be made between the two.\nStill confused? This video may help.\n\n\nF1 score\nF1 score is sometimes known as the harmonic mean of recall and precision. It’s an attempt to generalise the two. The range for this is between 0 to 1, the higher the better.\nIn phase 1 of the pxtextmining project, the metric that was optimised was class balance accuracy (not to be confused with balanced accuracy score).[1] This was a type of averaged accuracy score obtained across the different classes."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#multilabel-performance-metrics",
    "href": "pxtextmining/performance_metrics.html#multilabel-performance-metrics",
    "title": "Performance metrics",
    "section": "Multilabel performance metrics",
    "text": "Multilabel performance metrics\nIn a multilabel model, one sample can have more than one label. The exact number of labels assigned to the sample can vary. For example, we could label films like this:\n\n\n\nFilm\nLabels\n\n\n\n\nThe Mummy\nAction Adventure Comedy\n\n\nShrek\nComedy Animation\n\n\n\nPerformance measurement for models that perform multilabel classification is tricky. There is little consensus in the literature about which metric to select. This is because, as with the single-label example, it largely depends on what we’re trying to accomplish. The performance of the multi-label learning algorithm should therefore be tested on a broad range of metrics instead of only on the one being optimized.[2], [3] The current popular methods for reporting the performance of a multi-label classifier include the Hamming loss, precision, recall, and F-score for each class with the micro, macro, and weighted average of these scores from all classes.[4]\n\n\nTable 4: Example of multilabel scenario\n\n\nFilm\nReal Labels\nPredicted Labels\n\n\n\n\nThe Mummy\nAction Adventure Comedy\nAction Adventure\n\n\nShrek\nComedy Animation\nComedy Animation\n\n\n\n\n\nHamming loss\nThe Hamming loss is the fraction of labels that are incorrectly predicted. This ranges between 0 and 1, the lower the better. In the example in Table 4 above, there would be a Hamming loss of 0.125 as it only didn’t manage to capture 1 of the labels.\n\n\nJaccard score\nJaccard similarity index is the size of the intersection of the predicted labels and the true labels divided by the size of the union of the predicted and true labels. It ranges from 0 to 1, and 1 is the perfect score. However, if there are no true or predicted labels, the sklearn implementation of Jaccard will return an error.\n\n\nAveraging single label metrics\nAll of the usual classification metrics (Recall, Precision, F1 score) can be used in multilabel. For example, you could get a recall, precision, and F1 score for every label, on a one-vs-rest approach.\nIn multilabel, accuracy is sometimes also called ‘exact match accuracy’ or ‘subset accuracy’. This is quite a harsh metric, as all of the labels for the sample must be correct. In the example in Table 4, there would be a subset accuracy of only 0.5 as the model did not accurately predict all of the correct labels for the first sample, although it got 5 out of 6 labels right.\nYou could also obtain generalised recall, precision, and F1 scores for all the labels. The way that this works is the recall, precision, and F1 scores are obtained for each class, and then they are averaged out. They can be averaged in three ways: - Micro-averaged: all samples equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated differently according to the total true positives, false negatives and false positives). - Macro-averaged: all classes equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated equally in calculating the average) - Weighted-averaged: each classes’ contribution to the average is weighted by support (the number of true instances for each label)."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#conclusion-which-metrics-to-use-for-pxtextmining-phase-2",
    "href": "pxtextmining/performance_metrics.html#conclusion-which-metrics-to-use-for-pxtextmining-phase-2",
    "title": "Performance metrics",
    "section": "Conclusion: which metrics to use for pxtextmining Phase 2?",
    "text": "Conclusion: which metrics to use for pxtextmining Phase 2?\nUltimately there is not going to be one magic metric that is going to work for the pxtextmining project. As the dataset is quite imbalanced, and we do want to capture the less well-represented classes, the macro-averaged scores are likely to be helpful. I expect to use the macro-averaged F1 score as a very rough indication in the beginning stages of the project.\nHowever, per-class statistics will likely be most important for performance measurement and especially in the model tuning stage will be helpful for indicating labels that are proving hard to capture.[5] The Hamming and Jaccard scores will also be calculated to give an overall indicator of performance although I anticipate that these will not be the primary focus of model tuning. Unfortunately the metric ‘class balance accuracy’ which was utilised in phase 1 of the pxtextmining project cannot be used for the multilabel models in phase 2.\nDo note that there are other metrics available. The ones selected here are the ones that have been most commonly mentioned in the literature, as it makes sense to utilise what has been established as best practice elsewhere for generalisability and interpretability."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#a-note-on-baselinedummy-models",
    "href": "pxtextmining/performance_metrics.html#a-note-on-baselinedummy-models",
    "title": "Performance metrics",
    "section": "A note on baseline/dummy models",
    "text": "A note on baseline/dummy models\nWhen prototyping machine learning models, we need to first establish a baseline of performance - what is the most basic level that we should expect from our model? In the case of an imbalanced dataset, a model which just predicts the mode (the most frequently occurring class) regardless of the input would be a suitable dummy. Other approaches to ‘dummy’ models include a model that just outputs random predictions.\nIt is useful to know what performance a dummy model would be able to achieve, when considering the performance of any machine learning model. This gives us something for us to compare our outputs to."
  },
  {
    "objectID": "pxtextmining/performance_metrics.html#bibliography",
    "href": "pxtextmining/performance_metrics.html#bibliography",
    "title": "Performance metrics",
    "section": "Bibliography",
    "text": "Bibliography\n\nL. Mosley, ‘A balanced approach to the multi-class imbalance problem’, Doctor of Philosophy, Iowa State University, Digital Repository, Ames, 2013. doi: 10.31274/etd-180810-3375.\nM.-L. Zhang and Z.-H. Zhou, ‘A Review on Multi-Label Learning Algorithms’, IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819–1837, Aug. 2014, doi: 10.1109/TKDE.2013.39.\nX.-Z. Wu and Z.-H. Zhou, ‘A Unified View of Multi-Label Performance Measures’, in Proceedings of the 34th International Conference on Machine Learning, Jul. 2017, pp. 3780–3788. Accessed: Jan. 20, 2023. [Online]. Available: https://proceedings.mlr.press/v70/wu17a.html\nM. Heydarian, T. E. Doyle, and R. Samavi, ‘MLCM: Multi-Label Confusion Matrix’, IEEE Access, vol. 10, pp. 19083–19095, 2022, doi: 10.1109/ACCESS.2022.3151048.\nS. Henning, W. H. Beluch, A. Fraser, and A. Friedrich, ‘A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing’. arXiv, Oct. 10, 2022. Accessed: Jan. 20, 2023. [Online]. Available: http://arxiv.org/abs/2210.04675"
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Under Development"
  }
]